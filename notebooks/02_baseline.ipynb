{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H_3qtng1YbW"
   },
   "source": [
    "# TP3 - Parte 2: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-7NyhEs07CV"
   },
   "source": [
    "Vamos a construir un modelo muy sencillo para saber qué es lo peor que podemos hacer, en general esta es una tarea muy importante que queremos que repitan en sus proyectos de machine learning. ¿Por qué?\n",
    "\n",
    "- Navaja de Ockham: “Cuando se ofrecen dos o más explicaciones de un fenómeno, es preferible la explicación completa más simple; es decir, no deben multiplicarse las entidades sin necesidad.” ¿Para qué desarrollar un modelo super complejo si capaz es peor o casi igual que uno muy sencillo?\n",
    "- Nos sirve para saber si estamos usando bien los modelos más complejos, si su score nos da peor al baseline probablemente se deba a un error de código.\n",
    "- Nos sirve para rápidamente saber que tan complejo es un problema.\n",
    "- Los modelos simples son fáciles de entender.\n",
    "\n",
    "Se deben crear al menos dos features numéricas y dos features categóricas para entrenar una regresión logística, utilizando búsqueda de hiperparametros, realizando los encodings correspondientes y garantizando la reproducibilidad de los resultados cuando el notebook corriera varias veces. A su vez, usar un embedding para el campo text.\n",
    "\n",
    "Conteste las preguntas:\n",
    "\n",
    "- ¿Cuál es el mejor score de validación obtenido? (¿Cómo conviene obtener el dataset para validar?)\n",
    "- Al predecir con este modelo para la competencia, ¿Cúal es el score obtenido? (guardar el csv con predicciones para entregarlo después)\n",
    "- ¿Qué features son los más importantes para predecir con el mejor modelo? Graficar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Brxh-6Nn00dE"
   },
   "source": [
    "## Imports y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hakkOKQ6_9Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33K0JQGqCWux"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DLlSjtg7lzP"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/train_modificado.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763416056029,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "37jtcx9P7qSW",
    "outputId": "2f1204c2-3617-4f4c-b166-fd68ad922040"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTHz551f70pJ"
   },
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763416056034,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "IAg4RcK9_gzZ",
    "outputId": "0cae0ef7-8e41-49b7-b3e1-a4e771d13ab8"
   },
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763416056037,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "nvo-MKRT_g5U",
    "outputId": "3e333171-58be-494e-a8d2-cee4acd78d80"
   },
   "outputs": [],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ymv-rjB8nTT"
   },
   "source": [
    "## División en Train y Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd3F9jxUAH_J"
   },
   "source": [
    "Usando el parámetro stratify me aseguro que la proporción entre targets positivos y negativos se mantenga luego del split, igual voy a querer comprobarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKd-50YjKMbl"
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=13, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1763416056062,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "EbhAvNmTAzUQ",
    "outputId": "9c46c57c-0659-4d66-a7ce-f5754f6ba1e6"
   },
   "outputs": [],
   "source": [
    "print(\"Ratio original:\", df['target'].value_counts(normalize=True))\n",
    "print(\"Ratio y_train:\", y_train.value_counts(normalize=True))\n",
    "print(\"Ratio y_validation:\", y_validation.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jc_rTXMBHb5"
   },
   "source": [
    "Podemos ver que las proporciones se mantuvieron casi iguales por lo que nos aseguramos no confundir al modelo en este aspecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1763416056071,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "OteYKcFK_ZJl",
    "outputId": "440d8c7f-4ab1-4a68-cf7b-f442c7a7d77c"
   },
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1763416056071,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "d52fxEZiCTVY",
    "outputId": "2be08672-0e35-4f8f-ef80-4f2ee93fd0cc"
   },
   "outputs": [],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn22RYz6CiSg"
   },
   "source": [
    "## Escalado de los datos y embedding TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IImnBfLCmAS"
   },
   "source": [
    "Ahora que ya tenemos la separación en train y validation, podemos procesar train sin leakear los targets de validation lo que nos permite sacar features que no dependan unicamente de las otras features de la misma fila, sino también utilizando estadísticas de todo el set de train.\n",
    "\n",
    "Esto nos permitirá hacer un embedding TF-IDF sin leakear y_train y también escalar las features numéricas para que la regresión logística converga mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnmzI9orDJXn"
   },
   "source": [
    "## Escalado de las features numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYJgoXqY0swA"
   },
   "source": [
    "Las features numéricas que tenemos son:\n",
    "- location_count\n",
    "- tweet_length\n",
    "- words_count\n",
    "- num_uppercase_letters\n",
    "- num_uppercase_words\n",
    "- num_special_chars\n",
    "- num_digits\n",
    "\n",
    "Asique solo vamos a querer escalar estas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i42TF3ks2Nhx"
   },
   "outputs": [],
   "source": [
    "cols_numericas = [\n",
    "    \"tweet_length\",\n",
    "    \"words_count\",\n",
    "    \"num_uppercase_letters\",\n",
    "    \"num_uppercase_words\",\n",
    "    \"num_special_chars\",\n",
    "    \"num_digits\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCBaHwC7J9Jl"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMgFjIu8MKBq"
   },
   "source": [
    "Para escalar, necesito quedarme solo con las columnas numéricas y hacer el fit solo con train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763416056077,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "V2J47Y-FJ_Ao",
    "outputId": "7f56d8b5-2913-4497-dbb7-cb1a950c3292"
   },
   "outputs": [],
   "source": [
    "scaler.fit(X_train[cols_numericas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuVWzbHr2l0N"
   },
   "source": [
    "Ahora si, podemos hacer el transform y sobreescribir las columnas por los valores escalados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-MhW2GBKdXR"
   },
   "outputs": [],
   "source": [
    "X_train_scaled = X_train.copy()\n",
    "X_validation_scaled = X_validation.copy()\n",
    "\n",
    "X_train_scaled[cols_numericas] = scaler.transform(X_train[cols_numericas])\n",
    "X_validation_scaled[cols_numericas] = scaler.transform(X_validation[cols_numericas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1763416056123,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "hYAH1-MQ20Lc",
    "outputId": "2c50cdee-208c-402c-821e-a1b78bff190c"
   },
   "outputs": [],
   "source": [
    "X_train_scaled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1763416056126,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "UrzaiUG057R3",
    "outputId": "36f396c3-ad85-4d88-af72-b561d4155eed"
   },
   "outputs": [],
   "source": [
    "X_validation_scaled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHvrx12eM4b0"
   },
   "source": [
    "## Embedding TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRsmmTQRUGNG"
   },
   "source": [
    "### Eliminación de Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl7eIWc8P4EV"
   },
   "source": [
    "Para realizar este primer embedding que vamos a utilizar para nuestra regresión logística, ya podemos empezar a procesar el texto. El TfidfVectorizer de sklearn nos ahorra tener que hacer un tokenizado manual, pero puede verse afectado por la presencia de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PPVl2dyRItT"
   },
   "source": [
    "Por esto, vamos a eliminar primero las `stopwords` antes de aplicarlo. `TfidfVectorizer` tiene una opción para eliminar `stopwords` en su procesado, pero su [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=given%20callable%20analyzer.-,stop_words,-%7B%E2%80%98english%E2%80%99%7D%2C%20list%2C%20default) indica que su uso tiene algunos inconvenientes:\n",
    "\n",
    "> There are several known issues with ‘english’ and you should consider an alternative.\n",
    "\n",
    "Por lo tanto, vamos a utilizar la lista de `stopwords` que `nltk.corpus` para el idioma inglés. Nuestro trabajo previo con `langdetect` nos hizo observar que podemos asumir todos los tweets como entradas en ese idioma y por lo tanto no debemos preocuparnos por las `stopwords` de otros idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKWtg-JzStW7"
   },
   "source": [
    "El lado positivo es que corpus nos da una lista de palabras, y luego podemos pasar esa lista al parámetro de `TfidfVectorizer`:\n",
    "\n",
    "> If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if `analyzer == 'word'`.\n",
    "\n",
    "Esto funciona ya que el valor default de `analyzer` es efectivamente `'word'`. Además, no es case-sensitive ya que utilizaremos el parámetro `lowercase=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1763416056152,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "mCe0rQszTe1F",
    "outputId": "804b7872-5dd5-476e-9142-20fbb7bd4357"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mbx8-bSXS-TH"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1763416056170,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "ROZMYEXhS-3q",
    "outputId": "be85fbf8-250e-4cd6-8765-2523361fe54f"
   },
   "outputs": [],
   "source": [
    "stopwordsEng = list(stopwords.words('english'))\n",
    "stopwordsEng[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bCNxB1yUI-r"
   },
   "source": [
    "OBSERVACIÓN: Cómo no estamos tomando stopwords personalizadas según la frecuencia en nuestro dataset de train ni nada similar. Podríamos hacer este filtro directamente en el dataframe original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYWqIa7_gyqh"
   },
   "source": [
    "OBSERVACIÓN: Podriamos pensar que deberíamos haber computado las features anteriores con los tweets sin stopwords, y puede ser cierto. Aunque a su vez las features antes de eliminar las stopwords pueden tener información adicional relacionadas con la forma de escribir. Por falta de tiempo no haré un análisis comparando las dos posibilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAEQ7nMacuh-"
   },
   "source": [
    "### Vectorizado del Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAEe4aiivN5h"
   },
   "source": [
    "Para el vectorizado vamos a utilizar TweetTokenizer como tokenizador, que es un tokenizador especializado en tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFUE6Ec9M6Jz"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fuS6CX0sRZ_"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWsjg70Getol"
   },
   "source": [
    "Vamos a hacer un CountVectorizer previo con este tokenizador primero, simplemente porque quiero estimar cuál es la cantidad de features/palabras distintas en X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1763416056808,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "JZKXN7r9e84u",
    "outputId": "b5f9553b-920f-40ca-f1b2-a0c933b36154"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer(\n",
    "    lowercase = True,\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    token_pattern = None,\n",
    "    stop_words = stopwordsEng\n",
    ")\n",
    "v.fit(X_train[\"text\"])\n",
    "len(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5iFZD69fFOu"
   },
   "source": [
    "Tenemos Casi 20k tokens distintos, por lo tanto, voy a probar con algunos valores distintos de max_features a ver cuál me da el mejor resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef5EkRUNGLPv"
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = [\n",
    "    2000, 4000, 6000, 8000, 10000, 12000\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgXagp5xxFXT"
   },
   "source": [
    "Para iniciar, probamos solo con el primer valor y mostramos los pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC6PbtzxNCCY"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase = True,\n",
    "    max_features=MAX_FEATURES[0],\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    token_pattern = None,\n",
    "    stop_words = stopwordsEng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTVPjzK9f1DC"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVKq9QoJrB64"
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1763416058254,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "xsuvD4zwtTA1",
    "outputId": "0ebcec5d-b6fc-49de-d2e7-6712e33e7c9d"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf.toarray()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5cT_0h3tKou"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf_dense = pd.DataFrame(X_train_tfidf.toarray(), columns=feature_names, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1763416058609,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "eeqriMfRtd6T",
    "outputId": "cb8a79c0-aaa9-47c4-9620-5c2f1a4c3f9d"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf_dense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1763416058942,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "tByJ3SR_tt-f",
    "outputId": "150f8779-2975-4201-d274-15eee90d70a5"
   },
   "outputs": [],
   "source": [
    "pd.concat([X_train, X_train_tfidf_dense], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx81s1_6xKw_"
   },
   "source": [
    "Ahora juntamos todo esto en una función para poder aplicarlo automáticamente tanto a train como validation con los parámetros a elección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftGF9-L2oGb2"
   },
   "outputs": [],
   "source": [
    "def vectorizadoTfidf(X_train: pd.DataFrame, X_validation: pd.DataFrame, features: int, columna_texto: str, tokenizer_func=None):\n",
    "    X_train_copy = X_train.copy()\n",
    "    X_validation_copy = X_validation.copy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase = True,\n",
    "        max_features=features,\n",
    "        tokenizer=tokenizer_func,\n",
    "        token_pattern = None,\n",
    "        stop_words = stopwordsEng\n",
    "    )\n",
    "    vectorizer.fit(X_train_copy[columna_texto])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # train\n",
    "    train_transformed = vectorizer.transform(X_train_copy[columna_texto])\n",
    "    train_transformed_dense = pd.DataFrame(train_transformed.toarray(), columns=feature_names, index=X_train_copy.index)\n",
    "    train_full = pd.concat([X_train_copy, train_transformed_dense], axis=1)\n",
    "\n",
    "    # validation\n",
    "    validation_transformed = vectorizer.transform(X_validation_copy[columna_texto])\n",
    "    validation_transformed_dense = pd.DataFrame(validation_transformed.toarray(), columns=feature_names, index=X_validation_copy.index)\n",
    "    validation_full = pd.concat([X_validation_copy, validation_transformed_dense], axis=1)\n",
    "\n",
    "    return train_full, validation_full, vectorizer # Devuelvo el vectorizer por si quiero reutilizarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olC9cAaM5Maz"
   },
   "source": [
    "Ahora, podemos aplicarle el vectorizado a `X_train_scaled` que es donde ya escalamos, luego dropeamos las columnas que no nos sirven para el modelo (keyword, location y text) y podemos aplicar la regresión lineal.\n",
    "\n",
    "Podemos hacer esto en un loop para iterar sobre la cantidad de features en el vectorizado y comparar resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zpOtorz73QK"
   },
   "outputs": [],
   "source": [
    "def tokenizar_texto_tweet(text: str):\n",
    "  return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rGAP-OH7l04"
   },
   "outputs": [],
   "source": [
    "X_train_vectorizado, X_validation_vectorizado, vectorizer = vectorizadoTfidf(X_train_scaled, X_validation_scaled, 2000, \"text\", tokenizar_texto_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1763416061973,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "x7ku9nw28YQA",
    "outputId": "f103a83a-52d7-4594-b504-5244bcb23952"
   },
   "outputs": [],
   "source": [
    "X_train_vectorizado.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqMhQpKx81po"
   },
   "source": [
    "Finalmente dropeamos las columnas que no queremos y ya podemos aplicar la regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmptWOmb8kev"
   },
   "outputs": [],
   "source": [
    "X_train_final = X_train_vectorizado.drop(columns=['keyword','location','text'])\n",
    "X_validation_final = X_validation_vectorizado.drop(columns=['keyword','location','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zno7EjVh9QAT"
   },
   "source": [
    "## Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47aL9U7a9GLe"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CgrFO819GVK"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=250)\n",
    "# No uso max_iter como hiperparámetros porque no afecta al output, sino que limita cuantas iteraciones se realizan hasta convergencia.\n",
    "# El default es 100 asique lo seteo en 250 para seguridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weG3K2QOcNFP"
   },
   "source": [
    "Para la búsqueda de hiperparámetros, vamos a hacer un RandomSearch primero para acotar el espacio de posibles valores y acercarnos a algunas \"zonas\" mejores que otras. Y luego vamos a usar GridSearch con un set más reducido para quedarnos con lo mejor.\n",
    "\n",
    "Como LogisticRegression no tiene demasiados parámetros de espacio continuo (sino que la mayoría son opciones y espacios discretos), usaremos los que dieron mejor perfomance pero aprovecharemos para aplicar un GridSearch sobre max_iter. Este approach no aporta mucho más que un GridSearch y listo, pero lo vamos a hacer así ya que probablemente luego lo haremos igual con otros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHo8AuBRV0v6"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': loguniform(1e-3,100),\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'solver': ['lbfgs','saga'],\n",
    "    'class_weight': [None,'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAimDPAfV0tH"
   },
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=params,\n",
    "    n_iter=150,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6614953,
     "status": "ok",
     "timestamp": 1763422677189,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "8rLSeXtQW3or",
    "outputId": "2f5294de-c8ad-4a56-e423-a9cc69def6e9"
   },
   "outputs": [],
   "source": [
    "random_search.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1763422677205,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "aI8vezQsW3uB",
    "outputId": "82e4b3ea-2088-472b-ad24-33aaeb04724f"
   },
   "outputs": [],
   "source": [
    "print(\"Mejores hiperparámetros del RandomSearch:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"Mejor F1 obtenido (CV):\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BGIuSVgW3zF"
   },
   "outputs": [],
   "source": [
    "best_random_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Urvk7pv7cK5O"
   },
   "outputs": [],
   "source": [
    "C_best = best_random_params[\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Te0ErIXcK9u"
   },
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    \"C\": [C_best *0.25, C_best * 0.5, C_best * 0.75, C_best, C_best * 1.25, C_best * 1.5, C_best * 1.75, C_best * 2],\n",
    "    \"penalty\": [best_random_params[\"penalty\"]],\n",
    "    \"solver\": [best_random_params[\"solver\"]],\n",
    "    \"class_weight\": [best_random_params[\"class_weight\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKhkZF35cLCC"
   },
   "outputs": [],
   "source": [
    "lr_grid = LogisticRegression(max_iter=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3nPPWuCcLGB"
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_grid,\n",
    "    param_grid=grid_params,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 952151,
     "status": "ok",
     "timestamp": 1763423629413,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "-KTPyEExcLJG",
    "outputId": "5e9166d9-4cc4-438e-a4ba-28feb1659683"
   },
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1763423629434,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "2iZ7cQNAd8na",
    "outputId": "0a7c5101-a34f-4d15-a70a-53e4d2c5515b"
   },
   "outputs": [],
   "source": [
    "print(\"\\nMejores hiperparámetros finales (GridSearch):\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor F1 obtenido (CV):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nCwsmiWUd1W"
   },
   "source": [
    "- Surge la duda: Por qué el Mejor F1 obtenido de RandomSearch puede ser mejor que el Mejor F1 obtenido de GridSearch? Investigando encontré que como utilizamos Cross-Validation (cv=3), puede haber variaciones entre los folds usados por RandomSearch y GridSearch, que obtengan resultados distintos. Por eso hay esa pequeña variación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MiyQPcEeDfC"
   },
   "source": [
    "Nos quedamos entonces con el mejor estimador con los hiperparámetros buscados y usamos ese. Vamos a compararlo a su vez con la instancia por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-x7whP-d8rh"
   },
   "outputs": [],
   "source": [
    "lr_final = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 2822,
     "status": "ok",
     "timestamp": 1763423632260,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "ZDRzeXYD9Vjx",
    "outputId": "585ceedf-6458-4515-a544-15bbf6432d62"
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1763423632472,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "5_drzIc1eK2i",
    "outputId": "975867d3-e630-42fb-8aa9-995bfa4e4ff9"
   },
   "outputs": [],
   "source": [
    "f1 = f1_score(y_train, lr.predict(X_train_final))\n",
    "print(\"F1-score en train de lr default:\", f1)\n",
    "f1 = f1_score(y_train, lr_final.predict(X_train_final))\n",
    "print(\"F1-score en train de lr con hiperparámetros ajustados:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1763423632550,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "upUV6_Zdj6MN",
    "outputId": "369a0365-c407-46b0-8214-f9aa01380e7e"
   },
   "outputs": [],
   "source": [
    "lr_final.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1763423632550,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "a7-irFXGi-fb",
    "outputId": "2f949b57-e3a7-4604-d4a3-1ca778b89844"
   },
   "outputs": [],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCgWGbgW9oCh"
   },
   "source": [
    "Ahora podemos predecir sobre validación y medir el score con F1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1763425282284,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "fOJSHuqMVd11",
    "outputId": "bb030659-29c1-45df-b78a-a29b1f31e613"
   },
   "outputs": [],
   "source": [
    "f1 = f1_score(y_validation, lr.predict(X_validation_final))\n",
    "print(\"F1-score en validation de lr default:\", f1)\n",
    "f1 = f1_score(y_validation, lr_final.predict(X_validation_final))\n",
    "print(\"F1-score en validation de lr con hiperparámetros ajustados:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWv8gmU7_l6G"
   },
   "source": [
    "Tenemos un puntaje bastante potable. Pero vamos a barrer algunos thresholds hasta encontrar el umbral de corte para la regresión logistica que nos maximice el F1-score en validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dujGuOt_159"
   },
   "outputs": [],
   "source": [
    "probabilidades = lr_final.predict_proba(X_validation_final)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBkLrOrf-Weg"
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 200)\n",
    "f1_scores = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (probabilidades >= thr).astype(int)\n",
    "    f1_scores.append(f1_score(y_validation, y_pred_thr))\n",
    "\n",
    "best_thr = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "best_y_pred = (probabilidades >= best_thr).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1763423633291,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "t9O6cB4a_wD_",
    "outputId": "9c40cd50-d364-4ac7-b795-0be6e36d28ba"
   },
   "outputs": [],
   "source": [
    "print(\"Mejor threshold:\", best_thr)\n",
    "print(\"Mejor F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB9OeZgRAxgI"
   },
   "source": [
    "La verdad que un score así es mejor de lo esperado. Además, incluso habiedo ajustado los hiperparámetros, todavía podriamos iterar sobre distintos valores de MAX_FEATURES para ver cuál nos da un mejor resultado. Sin embargo por falta de tiempo voy a quedarme con este resultado para la Regresión Logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgdyLPBuBF5T"
   },
   "source": [
    "## Respondiendo Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--e2Xr6RBKd0"
   },
   "source": [
    "Pasamos ahora a contestar las preguntas de la consigna.\n",
    "\n",
    "1) ¿Cuál es el mejor score de validación obtenido? (¿Cómo conviene obtener el dataset para validar?)\n",
    "\n",
    "El mejor score obtenido fue 0.747 o casi 0.75. Para obtener el dataset para validar, lo que conviene es hacer un split que respete las distribuciones de targets positivos y negativos. Como no hay features temporales no debemos preocuparnos del time-travel. Además, hicimos el escalado y embedding de los features luego del split, lo que previene que haya data leaks respecto al target.\n",
    "\n",
    "Un mejor approach podría ser utilizar K-fold cross-validation para buscar que el modelo se comporte mejor ante nuevos features, pero no lo hice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tGsT_siCmrF"
   },
   "source": [
    "2) Al predecir con este modelo para la competencia, ¿Cúal es el score obtenido? (guardar el csv con predicciones para entregarlo después)\n",
    "\n",
    "Para predecir para la competencia, vamos a cargar test_modificado.csv en memoria, aplicar el escalado y embedding y luego realizar las predicciones con el modelo que obtuvo el mejor puntaje F1.\n",
    "\n",
    "Luego vamos a guardar las predicciones obtenidas con el mejor puntaje de validación en un .csv y luego vamos a subirlo a kaggle para ver el puntaje obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vSo_QZNFSdX"
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/processed/test_modificado.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvNDERkcFV9Q"
   },
   "outputs": [],
   "source": [
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[cols_numericas] = scaler.transform(X_test[cols_numericas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1763423633338,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "89iPns6RF63E",
    "outputId": "e7c6ae30-1bfe-4259-8f8c-15afe6a87d2f"
   },
   "outputs": [],
   "source": [
    "X_test_scaled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyL6V5g-HBQy"
   },
   "source": [
    "Reutilizamos el vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_OEsuJ5HCno"
   },
   "outputs": [],
   "source": [
    "test_transformed = vectorizer.transform(X_test_scaled['text'])\n",
    "test_transformed_dense = pd.DataFrame(test_transformed.toarray(), columns=feature_names, index=X_test_scaled.index)\n",
    "test_full = pd.concat([X_test_scaled, test_transformed_dense], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1763423634014,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "rcUQKlBHHBCj",
    "outputId": "f0cc0916-2b83-4444-ce18-d96af3377cb2"
   },
   "outputs": [],
   "source": [
    "X_test_final = test_full.drop(columns=['keyword','location','text'])\n",
    "X_test_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcQa40kkIspn"
   },
   "outputs": [],
   "source": [
    "probabilidades = lr_final.predict_proba(X_test_final)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcLcmagoHiv5"
   },
   "outputs": [],
   "source": [
    "test_pred = (probabilidades >= best_thr).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ9BKv8aHv6i"
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(test_pred, columns=['target'], index = X_test_final.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1763423634195,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "HgpMGJ5jJHDo",
    "outputId": "a5cfb158-6089-4f33-b8a5-33bb50ca247a"
   },
   "outputs": [],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1763423634232,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "ZydqKyWrJoTa",
    "outputId": "4eeda656-5694-4954-8ae6-a56109261a79"
   },
   "outputs": [],
   "source": [
    "submit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjEQaLkAALvr"
   },
   "outputs": [],
   "source": [
    "submit.to_csv(\"../data/processed/submit_baseline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4_FPf1rJVCD"
   },
   "source": [
    "Ahora subimos el submit a Kaggle y vemos el puntaje obtenido fue:\n",
    "> **Score: 0.78884**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-3mcNXJJ_bK"
   },
   "source": [
    "3) ¿Qué features son los más importantes para predecir con el mejor modelo? Graficar.\n",
    "\n",
    "Para un modelo de regresión logística, las importancias de las features provienen directamente de los coeficientes que se le asigna a cada una de ellas. Asique podemos obtenerlos, ordenarlos según su valor absoluto y graficarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKhXObUfJTxj"
   },
   "outputs": [],
   "source": [
    "coef = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3i9LdtyKeL3"
   },
   "outputs": [],
   "source": [
    "feature_names = X_train_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xu9XqDuKKkLy"
   },
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef': coef,\n",
    "    'abs_coef': np.abs(coef)\n",
    "}).sort_values('abs_coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhj6imVOKu_R"
   },
   "source": [
    "Podemos ver las 20 features más importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1763423634297,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "MMinrrnRKsj5",
    "outputId": "f20b5c2d-cfdf-4db2-bc42-9dbb5b42142e"
   },
   "outputs": [],
   "source": [
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lC7wFSIMK6On"
   },
   "source": [
    "Y podemos graficar las top_n features más importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1763423634704,
     "user": {
      "displayName": "Marcos Ezequiel Bat Mentzel",
      "userId": "10816598407518302379"
     },
     "user_tz": 180
    },
    "id": "aLLfvmXwKx_l",
    "outputId": "537fca86-2daa-4ecf-b89f-bbabbaf9d64e"
   },
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "colors = np.where(top_features['coef'] >= 0, \"seagreen\", \"darkred\")\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.barh(top_features['feature'], top_features['abs_coef'], color=colors)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importancia (|coef|)\")\n",
    "plt.title(f\"Top {top_n} features más importantes según Logistic Regression\")\n",
    "plt.text(0.95, 0.05, \"Colores:\\nCoef > 0 → seagreen\\nCoef < 0 → darkred\",\n",
    "         transform=plt.gca().transAxes,\n",
    "         fontsize=10,\n",
    "         ha='right',\n",
    "         bbox=dict(facecolor='white', alpha=0.7))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCNH755oLleb"
   },
   "source": [
    "Es interesante ver que, aún no ponderando las keywords. Muchisimas de las features más importantes provienen del embedding hecho sobre el texto y que muchas de ellas son palabras que probablemente sean keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvArPrPqMjvh"
   },
   "source": [
    "Más interesante aún, coloreando las barras según el signo del coeficiente, podemos ver que hay features/tokens del embedding que disminuyen el target haciendo menos probable que el tweet sea de una catástrofe. Tokens como 'love' tiene mucho sentido que tomen coeficientes negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX1_CdW6ObN0"
   },
   "source": [
    "## Persistencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRNBfwIwX7jk"
   },
   "source": [
    "El procesado extra de los datos no demoró mucho tiempo, y teniendo el código decidí hacer un nuevo collab de feature engineering para hacer un preprocesado de datos en limpio mucho mejor y completo. Así tendremos mejores features para los modelos más avanzados."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOz4gwS0CB2sbCoTDbJSDyV",
   "mount_file_id": "1zV63gPgB8fxLGsVBh-afT07bRMorcOPc",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
